{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f0b8682-348d-4445-b719-9f0b75abe4d1",
   "metadata": {},
   "source": [
    "# ws 04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f475aa-7e3b-4a13-9e9c-57b08cfabb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout #Conv2D,  MaxPool2D \n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f39746-c835-427a-8d1f-3197fb37c3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_img_shape=(128, 128) \n",
    "\n",
    "train_dir = './train_set'  \n",
    "\n",
    "val_dir = './val_set'  \n",
    "\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_set = train_datagen.flow_from_directory(train_dir,\n",
    "                                                 target_size=target_img_shape,\n",
    "                                                 batch_size=32, \n",
    "                                                 class_mode='sparse') \n",
    "\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input) \n",
    "val_set = val_datagen.flow_from_directory(val_dir,\n",
    "                                            target_size=target_img_shape,\n",
    "                                            batch_size=32,\n",
    "                                            class_mode='sparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eae7fed-d57d-420c-bb82-7ec550198eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set.class_indices)\n",
    "for image_batch, labels_batch in train_set:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    \n",
    "    img = image_batch[0] - image_batch[0].min()\n",
    "    img /= 275.0\n",
    "    \n",
    "    print('Min Max:', img.min(), img.max())\n",
    "    plt.imshow(img)\n",
    "\n",
    "    print('class:', labels_batch[0])\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe6ae0c-7ed9-4912-9706-dd8ff940ed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, counts = np.unique(train_set.classes, return_counts=True)\n",
    "\n",
    "print(ids)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa10ce64-b1a0-446a-a07a-cc62928e0996",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (train_set.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "labels   \n",
    "\n",
    "for i in ids:\n",
    "    print('{:>8} = {}' . format(labels[i], counts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca015b2e-68ac-47e5-a44c-fe7339c14ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [k for k in train_set.class_indices]\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9547ce08-8551-47c5-ab79-6303ffb6a95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train_labels = pd.DataFrame({'Label':label_names, 'Count':counts})\n",
    "df_train_labels.set_index('Label', inplace=True)\n",
    "df_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b3e9f-19b3-418a-bfe6-1cb5092440ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_labels.plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589986c9-49d1-4d1b-8001-b3073c11b244",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, counts = np.unique(val_set.classes, return_counts=True)\n",
    "\n",
    "print(ids)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211988d0-32c5-4f46-9586-744de6d8f852",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, train_count = np.unique(train_set.classes, return_counts=True)\n",
    "_, val_count = np.unique(val_set.classes, return_counts=True)\n",
    "\n",
    "print('Ratio Validation/Training set:', val_count/train_count * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aa04ed-da3a-43fb-bff8-4ace428dcba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set[0][0][0].min(),train_set[0][0][1].max())\n",
    "\n",
    "plt.imshow(train_set[0][0][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644746d8-6296-4e02-8475-cc02139950d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_shape = (target_img_shape[0], target_img_shape[1], 3)  # in_shape = (64,64,3)\n",
    "in_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7798ae-55ec-4287-acdd-b16e2d61e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = ResNet50(include_top=False, weights='imagenet',\n",
    "                   input_shape=in_shape)  \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(base_model)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))  \n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(128, activation='relu')) \n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(3, activation='softmax'))  \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07bbf67-61b2-492a-b269-03dcd3737468",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e4d6aa-5d1b-42ca-9a08-4b4d9776375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trainable..\\n---\")\n",
    "for variable in model.trainable_variables:\n",
    "    print(variable.name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f171d599-3e27-4a49-a701-274ecafbba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c935c72b-b22d-4c60-a51a-ae04f975a0bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', \n",
    "             metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', verbose=1, patience=5) # \n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit(train_set, \n",
    "                     validation_data=val_set, \n",
    "                    epochs=20, verbose=1, callbacks=[es, mc])\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time Taken: {:.2f} minutes\".format((end - start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279188da-ce53-4e58-bdab-b38fde5176bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3.5))\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='Training')\n",
    "plt.plot(history.history['val_loss'],'r', lw=3.2, label='Validation')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='Training')\n",
    "plt.plot(history.history['val_accuracy'], 'r', lw=3.2, label='Validation')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2b0b87-2468-4127-9634-a2fef202ba20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(val_set))\n",
    "acc = model.evaluate(val_set, verbose=1)\n",
    "print('score = {:.3f}' .format(acc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52df39f-bf1a-44a9-8b02-fca3669a978a",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97482721-6729-4181-9e0f-500dbf95b536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "def predict(img_fname):\n",
    "    img = load_img(img_fname, target_size=target_img_shape) \n",
    "    plt.imshow(img)\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "    \n",
    "    pred = model.predict(img)  \n",
    "    pred_cls = labels[np.argmax(pred, -1)[0]]   \n",
    "    print('Prediction:', pred_cls, pred[0].round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a05aaaf-e208-409a-b8ae-9f42bb230127",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('dog010-2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e8c577-0408-47e8-93ed-70a1df428dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('./elephant.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161a4cf1-11e8-4775-83a8-bd91d798bf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (train_set.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "labels  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cade32-da2f-4b3f-a4cb-ea41bda03eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[np.argmax(pred, -1)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a1db0d-0eac-4d06-8aa0-c1892a17df2e",
   "metadata": {},
   "source": [
    "# ws 05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00794bc-211f-420a-a575-725fadd56317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc16acf-f715-4c20-a259-f05fbe91f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_img_shape=(128, 128)  \n",
    "\n",
    "train_dir = './train_set'  \n",
    "\n",
    "val_dir = './val_set' \n",
    "\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input) \n",
    "\n",
    "\n",
    "train_set = train_datagen.flow_from_directory(train_dir,\n",
    "                                                 target_size=target_img_shape,\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='sparse')\n",
    "\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input) \n",
    "    \n",
    "val_set = val_datagen.flow_from_directory(val_dir,\n",
    "                                            target_size=target_img_shape,\n",
    "                                            batch_size=32,\n",
    "                                            class_mode='sparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0045b9de-a0a7-49ae-947b-fe4b583ac439",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set.class_indices)\n",
    "\n",
    "\n",
    "for image_batch, labels_batch in train_set:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    \n",
    "    img = image_batch[0] - image_batch[0].min()\n",
    "    img /= 2.0\n",
    "   \n",
    "    print(img.min(), img.max())\n",
    "    plt.imshow(img)\n",
    "\n",
    "    print('class:', labels_batch[0])    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1d285a-e6a3-4d19-9715-e8293b6e76f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_shape = (target_img_shape[0], target_img_shape[1], 3)  \n",
    "\n",
    "\n",
    "base_model = MobileNetV2(include_top=False, weights='imagenet',\n",
    "                   input_shape=in_shape)  \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(base_model)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))  \n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(128, activation='relu'))  \n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(3, activation='softmax'))  \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e314a910-fd30-4f11-b86f-204fb77a9e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
